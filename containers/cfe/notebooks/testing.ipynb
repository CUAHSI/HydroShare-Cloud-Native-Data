{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77bc73-1262-43db-b553-f142465dc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dask\n",
    "import typer\n",
    "import numpy\n",
    "import shutil\n",
    "import pyproj\n",
    "import xarray\n",
    "import pandas\n",
    "import logging\n",
    "import rioxarray\n",
    "import geopandas\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "from geocube.api.core import make_geocube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6483dcb-1c65-4087-a862-c6d818ac6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004969dc-aab4-4615-8581-9635bd29a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=4, memory_limit='4GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e58ea-68eb-4127-a065-1f95c9a6ea4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(forcing, geopackage):  \n",
    "    ds = load_ds(forcing)\n",
    "    gdf = load_gdf(geopackage)\n",
    "    return ds, gdf\n",
    "\n",
    "def load_ds(forcing):  \n",
    "    \n",
    "    # load forcing data\n",
    "    ds = xarray.open_dataset(forcing)\n",
    "    \n",
    "    return ds\n",
    "    \n",
    "def load_gdf(geopackage):    \n",
    "    \n",
    "    \n",
    "    # load hydrofabric\n",
    "    gdf = geopandas.read_file(geopackage, layer='divides')\n",
    "    \n",
    "    # convert these data into the projection of our forcing data\n",
    "    # this assumes that we're using AORC forcing.\n",
    "    # TODO: generalize this to use whatever projection is defined in the \n",
    "    # forcing dataset\n",
    "    target_crs = pyproj.Proj(proj='lcc',\n",
    "                             lat_1=30.,\n",
    "                             lat_2=60., \n",
    "                             lat_0=40.0000076293945, lon_0=-97.,\n",
    "                             a=6370000, b=6370000)\n",
    "    gdf = gdf.to_crs(target_crs.crs)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "@dask.delayed\n",
    "def prepare_zonal(in_ds, gdf):\n",
    "\n",
    "    # create zonal id column\n",
    "    gdf['cat'] = gdf.id.str.split('-').str[-1].astype(int)\n",
    "\n",
    "    # set the aorc crs.\n",
    "    # TODO: This should be set when the dataset is saved, not here.\n",
    "    in_ds =  in_ds.rio.write_crs('EPSG:4326', inplace=True)\n",
    "    \n",
    "    # create a grid for the geocube\n",
    "    out_grid = make_geocube(\n",
    "        vector_data=gdf,\n",
    "        measurements=[\"cat\"],\n",
    "        like=in_ds # ensure the data are on the same grid\n",
    "    )\n",
    "\n",
    "    # add the catchment variable to the original dataset\n",
    "    in_ds = in_ds.assign_coords(cat = (['latitude','longitude'], out_grid.cat.data))\n",
    "\n",
    "    return in_ds\n",
    "\n",
    "@dask.delayed \n",
    "def delayed_zonal_computation(ds):\n",
    "    return ds.groupby(ds.cat).mean()\n",
    "    \n",
    "    #d = ds.where(ds.cat==cat_id, drop=True)\n",
    "    #res =  {variable: ds.mean(dim=['x','y']).values}\n",
    "    #return d.mean(dim=['x','y']).resample(time=\"1h\").sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74d9f7-5aad-4254-8e17-bdc16cc14e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "geopackage = 'input-data/wb-2917533_upstream_subset.gpkg'\n",
    "forcing = 'input-data/results.nc'\n",
    "output_data = 'output-data'\n",
    "\n",
    "results = []\n",
    "\n",
    "ds, gdf = load_data(forcing, geopackage)\n",
    "#ds = ds.isel(time=range(0,100))\n",
    "scattered_ds = client.scatter(ds, broadcast=True)\n",
    "scattered_gdf = client.scatter(gdf, broadcast=True)\n",
    "\n",
    "zonal_ds = prepare_zonal(scattered_ds, scattered_gdf).compute()\n",
    "scattered_zonal_ds = client.scatter(zonal_ds, broadcast=True)\n",
    "\n",
    "# clean up\n",
    "del scattered_ds\n",
    "del scattered_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f0029-9b61-4ec9-9848-372af90bcbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = delayed_zonal_computation(scattered_zonal_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093df78-a7bc-4122-be30-3544ca164fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = r.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533208c6-52d2-4a4c-bad7-fcd2d81b79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def save_to_csv(results, cat_id, output_dir):\n",
    "    fname = f'cat-{int(cat_id)}'\n",
    "    with open(f'{output_dir}/{fname}.csv', 'w') as f:\n",
    "        df = results.sel(dict(cat=cat_id)).to_dataframe()\n",
    "        df.fillna(0., inplace=True)\n",
    "        df['APCP_surface'] = df.APCP_surface * 3600\n",
    "        df.to_csv(f, columns = ['APCP_surface',\n",
    "                                'DLWRF_surface',\n",
    "                                'DSWRF_surface',\n",
    "                                'PRES_surface',\n",
    "                                'SPFH_2maboveground',\n",
    "                                'TMP_2maboveground',\n",
    "                                'UGRD_10maboveground',\n",
    "                                'VGRD_10maboveground'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d8a84-f868-4d7e-b10f-4c899dc3b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cat from float to string\n",
    "results = results.assign_coords({'cat': results.cat.astype(int).astype(str)})\n",
    "results_scattered = client.scatter(results, broadcast=True)\n",
    "\n",
    "delayed_write = []\n",
    "for cat in results.cat.values:\n",
    "    delayed = save_to_csv(results_scattered, cat, output_data)\n",
    "    delayed_write.append(delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205288a0-94d0-4c0e-85b5-e17ce9ba2641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = dask.compute(delayed_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cee4c3-54a1-4eb8-8b32-3c6daa0b761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b57a7-9b92-4cc7-a2b0-f3908bb0d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ds.time.values.min()\n",
    "et = ds.time.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fe7c3-433e-43fc-ac0a-d10bc2399c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "((et-st).item() * 10**-9) / (3600*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3218fab-078c-4efc-8e27-4d01c3a95089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77a6a0-4f76-4ca8-8315-f13b67a9e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = pandas.to_datetime(ds.time.values.min())\n",
    "et = pandas.to_datetime(ds.time.values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01d3ce-96f4-4db9-a2b3-f75d5ff87cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(et-st).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417cada-24e3-4bd2-aa43-b1a84192fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_catchments = list(ds.cat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebadce-b26e-4b5c-bf7e-77b36d494098",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_catchments = gdf.id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380ea59-0922-4159-b8b4-e970179de8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff =  len(known_catchments) - len(computed_catchments)\n",
    "if diff > 0:\n",
    "    print(f'{diff} catchments missing from NGen Subset.\\nComputing synthetic data for these')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de13f5-ca50-4789-8934-1ec16bc8b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for known_id in known_catchments:\n",
    "    _id = known_id.split('-')[-1]\n",
    "    if _id not in computed_catchments:\n",
    "        print(f'missing {_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a669946-77a4-4b63-99f8-d36b47897843",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_catchments[0].split('-')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3bcb5-cb7b-4f7d-b575-cdc02122ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de616a5f-97e8-4a81-8379-8988fc379ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
