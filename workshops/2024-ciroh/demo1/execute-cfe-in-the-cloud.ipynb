{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CFE Simulation in the Cloud\n",
    "\n",
    "**Authors**\n",
    "\n",
    "- Tony Castronova: acastronova@cuahsi.org\n",
    "- Irene Garousi-Nejad: igarousi@cuahsi.org\n",
    "- Scott Black: sblack@cuahsi.org\n",
    "\n",
    "**Last Updated:** 05.17.2023\n",
    "\n",
    "**Description**\n",
    "\n",
    "This notebook demonstrates how to build and execute scientific workflows in the cloud using cyberinfrastructure developed as part of the \"HydroShare Modernization\" CIROH research project. The goal is do illustrate how the general-purpose cloud analysis workflows that have been developed to support common data archival operations, can also be leveraged for scientific computing. This notebook describes the process for using the outcomes of the aforementioned CIROH project, however these capabilities are still under active development are not ready for wide-spread public use.\n",
    "\n",
    "\n",
    "**Data Availability**\n",
    "\n",
    "This notebook requires the NextGen Hydrofabric that is hosted on http://lynker-spatial.com. These data are subdivided into a series of GeoPackage files grouped relative to the portion of the USA in which they are located.. Data can be accessed in a variety of ways, the simplest being a direct download:\n",
    "\n",
    "```\n",
    "wget https://lynker-spatial.s3-us-west-2.amazonaws.com/hydrofabric/v20.1/gpkg/nextgen_16.gpkg\n",
    "```\n",
    "\n",
    "**Computational Availability**\n",
    "\n",
    "This notebook leverages advanced cyberinfrastructure that is currently under active development. It has been made available to attendees that the 2024 CIROH User and Developer Conference, however it is not currently available to the general public. Access to this system may be terminated without notice.\n",
    "\n",
    "\n",
    "**Software Requirements**\n",
    "\n",
    "```\n",
    "- fiona==1.9.6  \n",
    "- fsspec==2024.5.0  \n",
    "- geopandas==0.14.4  \n",
    "- ipyleaflet==0.18.2  \n",
    "- matplotlib==3.9.0  \n",
    "- numpy==1.26.4  \n",
    "- pandas==2.2.2  \n",
    "- pyproj==3.6.1  \n",
    "- s3fs==2024.5.0  \n",
    "- shapely==2.0.4  \n",
    "- sidecar==0.7.0  \n",
    "- swagger-client @ git+https://github.com/CUAHSI/argo-workflow-python-client.git@954441c4dc47a05e80bd5eb1186f3723a4863d8d  \n",
    "- tabulate==0.9.0  \n",
    "- xarray==2024.5.0  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "import fsspec\n",
    "import utils\n",
    "import geopandas\n",
    "import ipyleaflet\n",
    "from datetime import datetime\n",
    "from helpers import ArgoAPI, SideCarMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interact with the CUAHSI Argo service, we need to set our `ARGO_TOKEN`. This is obtained directly from the [Argo interface](https://workflows.argo.cuahsi.io/userinfo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGO_TOKEN='Bearer v2:eyJhbGciOiJSU0EtT0FFUC0yNTYiLCJlbmMiOiJBMjU2R0NNIiwiemlwIjoiREVGIn0.ebroHQI1YODONd-ZcGwBOXeFCwbbAKU8Adj3yjc5-9F9c0rLdk-dsBbb3vmWlsk_nMfLcb5NdfM1XQzOiwDCpiN21F2K8nwOX1fZRiSlqx0ojVcNlMhMLjtYVr-R9-2cV8-nadUvO5Vy9HY4aKvy4PK96HZF-ftVCucqGXrKQUnf3V0mRkn8K5l20HdlhKX8iYabnqjlBrhdFtbH68WJ7ja_cBEUpPKgmSE4RaM6llJNFOxQ-BWsquPSfNvSNFxl0muedqB5ISBjHndPh4dupvP_GjzQ1KchLYB1-S_a8oaNth_YQFTQnnwIru1TKm36AG0pskU-EEema0kTJjzWXw.17OrJT7nDaPQegFw.kimntRuzTjqk52qmXLWG7Kds1_8YLK1LWxPaLsfK9Rjmjwf5pmkZNli-MQl_QiKXLNPc2-UQbBI1dTI_XYPMY8tBfBuBwqr_UtQKwL6zl4TgMYiiKIHwcWPhgbHirEg2b8jvXL59l3nynEiRRPKYQCuAFbmzN349waHnSGDSkP2fWYfcQ_kRsasgC5Ma3JUXeYlJqIcoSUCF8SwGKGB_TdfZloX1wbtD2EgpvqWXjQfzsNiw7P9FwrWmD1pIiuoLuKkKI_7sHWaliHqnZsNGMleUxVMx15RsEg.1PQLXCHKGBBmN6TrqViYNg' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the `ArgoAPI` class. This will help keep our notebook clean by wrapping Argo REST API function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of ArgoAPI using our ARGO_TOKEN\n",
    "argo = ArgoAPI(ARGO_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the workflows that are available for us to use. Each of the workflows listed below are actually *templates* that can be modified with custom input and output parameters. Each defines a single task or a set of tasks that will be performed in a specific order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argo.list_workflows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn more about these workflows by looking at their metadata. The `helper` library contains some functions to display workflow metadata in a clear manner. It's important that we understand what these are so that we can successfully invoke a workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the metadata for the workflow\n",
    "argo.describe('run-cfe-complete-new-path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this workflow, we need to provide values for each of the input parameters listed above. While some of these are obvious others are more nondescript, for example `wb-id` and `vpu-id`. These are attributes obtained from the NextGen Hydrofabric. To simplify the process of acquiring these, we'll use a map interface provided in the `helpers.py` file.\n",
    "\n",
    "The data that we'll be using this notebook are located inside Vector Processing Unit 16. This area of the Hydrofabric covers the Great Basin. See the [NextGen Hydrofabric](https://mikejohnson51.github.io/hyAggregate/) help pages for more information regarding these data. A geopackage file may consist of many layers. Use `Fiona` to view the vector layers that are included within the `nextgen_16` geopackage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the layers include in the nextgen geopackage for VPU 16.\n",
    "fiona.listlayers('nextgen_16.gpkg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow will require the ID of the outlet catchment, so let's load the 'divides' layer. This layer contains over 30,000 features so we will not attempt to display it in the notebook. For reference, these data cover the following area:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vpu-16.png](./img/vpu-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load these data into `Geopandas` and convert into the coordinate reference we will be using in out leaflet map (EPSG: 4326)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.read_file('nextgen_16.gpkg', layer='divides')\n",
    "gdf = gdf.to_crs(epsg='4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch an interactive map interface for us to select our area of interest. We'll pass the `divides` geometries that were loaded above so we can query them when the map is clicked and retrieve their metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SideCarMap(gdf=gdf)\n",
    "m.display_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map contains USGS river gauges and NHD+ reach geometries to help us select our area of interest. After selecting our outlet catchment we can access it's metadata in the notebook using the `m.selected()` command. We're interested in the `id` attribute of this feature. It should look something like: `wb-2853613`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_catchment = m.selected()\n",
    "selected_catchment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've selected our feature-of-interest, let's prepare the remaining input parameters needed for workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_id         = selected_catchment.id  # the id of the selected catchment on the map.\n",
    "vpu_id        = 16                     # the VPU ID of the data we loaded, i.e. 16.\n",
    "output_bucket = 'scratch'              # this is a bucket specifically created for this workshop.\n",
    "start_date    = '2018-01-01'           # the start date of our simulation.\n",
    "end_date      = '2020-12-01'           # the end date of our simulation.\n",
    "\n",
    "# define input parameters for the job\n",
    "parameters = {'wb-id'        : wb_id,\n",
    "              'vpu-id'       : vpu_id,\n",
    "              'output-bucket': output_bucket,\n",
    "              'start-date'   : start_date,\n",
    "              'end-date'     : end_date}\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the workflow!\n",
    "\n",
    "The output of this function is a unique identifier for the job. We'll save this to the `job_name` variable so that we can use it later to interact with the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = argo.submit_workflow('run-cfe-complete-new-path', parameters)\n",
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the status of our running job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def collect_nodes(w, name, d):\n",
    "    \n",
    "    node = w.status.nodes[name]\n",
    "    children = node.children\n",
    "    if node.display_name[-3:] != '(0)':    \n",
    "        d[node.display_name] = node.id \n",
    "    if children is not None:\n",
    "        for child in children:\n",
    "            collect_nodes(w, child, d)\n",
    "    return d\n",
    "\n",
    "def print_workflow_status(job_name):\n",
    "    w = argo.workflow_api_instance.workflow_service_get_workflow(argo.namespace, job_name)\n",
    "    job_ids = collect_nodes(w, job_name, {})\n",
    "    still_running = True\n",
    "    statuses = []\n",
    "    for display_name, name in job_ids.items():\n",
    "        node = w.status.nodes[name]\n",
    "        statuses.append(node.phase)\n",
    "        if node.phase == 'Succeeded':\n",
    "            st = datetime.strptime(node.started_at, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            et = datetime.strptime(node.finished_at, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            print(f'{display_name}: {node.phase} -> {(et-st).total_seconds():.2f} seconds')\n",
    "        else:\n",
    "            print(f'{display_name}: {node.phase}')\n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------')\n",
    "print('View the workflow in the Argo UI:')\n",
    "print('---------------------------------')\n",
    "print(f'https://workflows.argo.cuahsi.io/workflows/workflows/{job_name}\\n')\n",
    "\n",
    "print('-------------------')\n",
    "print('Current Job Status:')\n",
    "print('-------------------')\n",
    "stats = print_workflow_status(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preview the output data that our job created, we need to first construct the url to our output data. Our output path use our hydroshare username so that we can easily find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydroshare_username = 'tonycastronova'\n",
    "url = f's3://{output_bucket}/{hydroshare_username}/{wb_id}/{job_name}'\n",
    "\n",
    "print(f'Browse the output files at:')\n",
    "print(f'https://console.minio.cuahsi.io/browser/{output_bucket}/{hydroshare_username}/{wb_id}/{job_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# connect to the CUAHSI MinIO server that is hosting our data\n",
    "s3 = fsspec.filesystem(\"s3\",\n",
    "                       anon=True,\n",
    "                       client_kwargs={'endpoint_url':'https://api.minio.cuahsi.io'},\n",
    "                       use_listings_cache=False,\n",
    "                      )\n",
    "\n",
    "def pretty_list(url, indent_count=0):\n",
    "    items = s3.listdir(url)\n",
    "    for item in items:\n",
    "        if item['type'] == 'directory':\n",
    "            indent = ' '*indent_count\n",
    "            name = Path(item['name']).name\n",
    "            print(f'{indent}+ {name}')\n",
    "            next_indent_count = indent_count + 1\n",
    "            pretty_list(item['Key'], next_indent_count)\n",
    "        else:\n",
    "            if item['type'] == 'file':\n",
    "                indent = ' '*indent_count\n",
    "                name = Path(item['name']).name\n",
    "                print(f'{indent}- {name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_list(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview HydroFabric on the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsetted NextGen HydroFabric data is located in the `/domain` directory. Let's load the Geopackage file that contains the hydrofabric geometries that were used in our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_url=f'{url}/domain/{selected_catchment.id}_upstream_subset.gpkg'\n",
    "gpkg_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Geopackage file contains several layers. Lets load the catchment divides and nexus points and convert them into the coordinate reference system used in the map interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_divides = geopandas.read_file(s3.open(gpkg_url), layer='divides')\n",
    "subset_divides = subset_divides.to_crs(epsg='4326')\n",
    "\n",
    "subset_nexus = geopandas.read_file(s3.open(gpkg_url), layer='nexus')\n",
    "subset_nexus = subset_nexus.to_crs(epsg='4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load these geometries on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add catchment divides to the map\n",
    "for idx, row in subset_divides.iterrows():\n",
    "\n",
    "    wlayer = ipyleaflet.WKTLayer(\n",
    "        wkt_string=row.geometry.wkt,\n",
    "            style={'color': 'red', 'opacity':1, 'weight':2.,})\n",
    "    m.map.add(wlayer)\n",
    "\n",
    "# add nexus points to the map\n",
    "for idx, row in subset_nexus.iterrows():\n",
    "\n",
    "    wlayer = ipyleaflet.WKTLayer(\n",
    "        wkt_string=row.geometry.wkt,\n",
    "            style={'color': 'black', 'opacity':1, 'weight':2.,})\n",
    "    m.map.add(wlayer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Simulation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and forcing data are locating the directories `/results` and `/forcing`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = f'{url}/results'\n",
    "forcing_path = f'{url}/forcing'\n",
    "print(f'forcing_path: {forcing_path}\\nresults_path: {results_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data associated with our most downstream catchment. First we need to identify the divide and nexus identifier for this catchment, then load the simulated results using Xarray. The latter is done with a utility library to keep our notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the catchment and nexus ids associated with the most downstream catchment, i.e. the one that was used as input to our workflow.\n",
    "outlet_cat_id = selected_catchment.divide_id\n",
    "outlet_nex_id = selected_catchment.toid\n",
    "print(f'cat-id: {outlet_cat_id}, nex-id: {outlet_nex_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the catchment output results\n",
    "cat_csv_files = s3.glob(f'{results_path}/cat*.csv')\n",
    "nex_csv_files = s3.glob(f'{results_path}/nex*.csv')\n",
    "forcing_csv_files = s3.glob(f'{forcing_path}/cat*.csv')\n",
    "\n",
    "xr_nex = utils.nex_csv2xr(s3, nex_csv_files)\n",
    "xr_cat = utils.cat_csv2xr(s3, cat_csv_files)\n",
    "xr_forcing = utils.forcing_csv2xr(s3, forcing_csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot of simulated streamflow and input preciptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_nex_sub = xr_nex.sel(time=slice('2019-01-01', '2020-01-01'))\n",
    "xr_cat_sub = xr_cat.sel(Time=slice('2019-01-01', '2020-01-01'))\n",
    "xr_forcing_sub = xr_forcing.sel(time=slice('2019-01-01', '2020-01-01'))\n",
    "\n",
    "utils.plot_precip_and_flow(outlet_cat_id,\n",
    "                           outlet_nex_id,\n",
    "                           xr_cat_sub,\n",
    "                           xr_nex_sub,\n",
    "                           xr_forcing_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output the paths to our data so we can reference these in the future. This is particularly useful for when we want to make modifications to our simulation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Bucket Name: \\t {output_bucket}')\n",
    "print(f'Data Path: \\t {hydroshare_username}/{wb_id}/{job_name}/')\n",
    "print(f'MinIO Path: \\t https://console.minio.cuahsi.io/browser/{output_bucket}/{hydroshare_username}/{wb_id}/{job_name}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
