{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebce2d9-fdd2-4ac6-8a29-2a9ad1a91f60",
   "metadata": {},
   "source": [
    "# Modify Parameters and Run CFE Simulation in the Cloud\n",
    "\n",
    "**Authors**\n",
    "\n",
    "- Irene Garousi-Nejad: igarousi@cuahsi.org\n",
    "- Tony Castronova: acastronova@cuahsi.org\n",
    "- Scott Black: sblack@cuahsi.org\n",
    "\n",
    "**Last Updated:** 05.28.2023\n",
    "\n",
    "**Description**\n",
    "\n",
    "This notebook demonstrates how to build and execute scientific workflows in the cloud using cyberinfrastructure developed as part of the \"HydroShare Modernization\" CIROH research project. The goal is do illustrate how the general-purpose cloud analysis workflows that have been developed to support common data archival operations, can also be leveraged for scientific computing. This notebook describes the process for using the outcomes of the aforementioned CIROH project, however these capabilities are still under active development are not ready for wide-spread public use.\n",
    "\n",
    "\n",
    "**Data Availability**\n",
    "\n",
    "This notebook requires access to the complete hydrological simulation inputs and outputs.\n",
    "\n",
    "**Computational Availability**\n",
    "\n",
    "This notebook leverages advanced cyberinfrastructure that is currently under active development. It has been made available to attendees that the 2024 CIROH User and Developer Conference, however it is not currently available to the general public. Access to this system may be terminated without notice.\n",
    "\n",
    "\n",
    "**Software Requirements**\n",
    "\n",
    "- git+https://github.com/CUAHSI/argo-workflow-python-client.git\n",
    "- geopandas-0.14.4\n",
    "- fiona-1.9.6\n",
    "- numpy-1.26.4\n",
    "- pandas-2.2.2\n",
    "- pyproj-3.6.1\n",
    "- shapely-2.0.4\n",
    "- ipyleaflet-0.19.1\n",
    "- sidecar-0.7.0\n",
    "- fsspec-2024.5.0\n",
    "- s3fs-2024.5.0\n",
    "- pandas-2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956d38f-6ab5-4418-913a-5ee04048c6c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25ad6f-ec21-4705-822f-1bfa040f55f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import geopandas\n",
    "from helpers import ArgoAPI, SideCarMap\n",
    "import fiona\n",
    "import fsspec\n",
    "import ipyleaflet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514c8bb-fd5b-45fb-9387-b259b71d8385",
   "metadata": {},
   "source": [
    "### Access data on Minio and Preview Domain Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858ce4c-8806-494e-b945-a147ac18493c",
   "metadata": {},
   "source": [
    "Bring the outputs of the last code cell from Demo 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30877b-c806-41be-8216-99d4475d29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=\"\"\n",
    "data_path=\"\"\n",
    "MinIO_Path = \"\"\n",
    "Outlet_catchment_id=\"\"\n",
    "Outlet_nexus_id=\"\"\n",
    "\n",
    "print(MinIO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710cfb86-b4bc-4c34-91ce-4e26777ace92",
   "metadata": {},
   "source": [
    "Initialize a filesystem object using the `fsspec` library, which provides a unified API for working with various file systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d7e33-dcf9-4a66-9891-ef5dd4a83271",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = 'https://api.minio.cuahsi.io'\n",
    "fs = fsspec.filesystem('s3', client_kwargs={'endpoint_url': endpoint_url}, anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568b711-f6d2-4684-abcb-b32a39aab34e",
   "metadata": {},
   "source": [
    "Access and read various domain-related data files stored in an S3 bucket using the `fsspec` library to interact with the S3 storage, and `geopandas` and `pandas` libraries to read geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5d075-e6d9-464f-b4fc-39356604616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data stored in an S3 bucket (full s3 url)\n",
    "base_path = f's3://{bucket_name}/{data_path}'\n",
    "\n",
    "# Define path to domain data\n",
    "catchments_path = base_path + 'domain/' + 'catchments.geojson'\n",
    "attributes_path = base_path + 'domain/' + 'cfe_noahowp_attributes.csv'\n",
    "flowpaths_path = base_path + 'domain/' + 'flowpaths.geojson'\n",
    "nexus_path = base_path + 'domain/' + 'nexus.geojson'\n",
    "gpkg_path = base_path + 'domain/' + f'{data_path.split(\"/\")[1]}_upstream_subset.gpkg'\n",
    "\n",
    "# Read data\n",
    "catchments = geopandas.read_file(fs.open(catchments_path))\n",
    "flowpaths = geopandas.read_file(fs.open(flowpaths_path))\n",
    "nexus = geopandas.read_file(fs.open(nexus_path))\n",
    "attributes = pd.read_csv(fs.open(attributes_path))  # cfe_noahowp_attributes.csv\n",
    "\n",
    "with fs.open(gpkg_path) as f:\n",
    "    gdf_flow = geopandas.read_file(f, layer='flowpaths')\n",
    "    f.seek(0)  # Reset file pointer\n",
    "    gdf_cat = geopandas.read_file(f, layer='divides')\n",
    "    f.seek(0)  # Reset file pointer\n",
    "    gdf_nexus = geopandas.read_file(f, layer='nexus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbc514-598b-4885-adaa-4ff2a8d03bb7",
   "metadata": {},
   "source": [
    "Print catchments and attributes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63298553-6591-4cb2-bcf3-3fa4cc451360",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b26e0-7ffc-464b-bfeb-1a6c3c691cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546efcf7-a388-4987-869b-2606ffd308c8",
   "metadata": {},
   "source": [
    "combine two DataFrames (`catchments` and `attributes`) based on a common column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef1854-3240-447c-8a14-da7631a6563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = catchments.merge(attributes, on='divide_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726e3f7-70fe-417b-a8bf-5bfe2d2c2b62",
   "metadata": {},
   "source": [
    "Let's now plot the histogram of a parameter of interest across the domain. Then, we will highlight a catchment of interest and show the value of the parameter for the selected catchment on the histogram. You can choose any catchment from merged['divide_id']. For example, we can use the outlet catchment ID referenced in Demo 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae941820-e922-4ee0-b58c-2d1dbd9e4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'refkdt'                      # reference hydraulic conductivity\n",
    "cat = Outlet_catchment_id              # catchment ID\n",
    "\n",
    "utils.plot_single_cat(merged, param, 'Infiltration Scaling Parameter', 'viridis', cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782d4c7-161d-47f4-84c9-e5b87c45d024",
   "metadata": {},
   "source": [
    "Print the parameter value for the catchment of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc25948-0c72-43af-938c-e1283b737c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{param} for {cat}: {merged.loc[merged[\"divide_id\"]==cat].refkdt.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccfff2-95d4-4943-b9ea-acadf706de1f",
   "metadata": {},
   "source": [
    "We can also find this value in the configuration file for the selected catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f2b16-6fbf-4350-9511-99753936da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = base_path + 'config/' +  f'{cat}_config.ini'\n",
    "config_dict = utils.load_config(config_path, endpoint_url, fs)\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46997d53-9e23-49ef-935f-e35c32f7f95b",
   "metadata": {},
   "source": [
    "### Modify Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56783841-0a29-4efa-953b-041d8e4bfcdc",
   "metadata": {},
   "source": [
    "**refkdt** is actually runoff/infiltration rate and you can learn more about it in this article. This parameter is manually calibrated over multiple simulations and significantly impacts surface infiltration and hence the partitioning of total runoff into surface and subsurface runoff. <strong>Increased values of REFKDT leads to more infiltration and less surface runoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634ae5-9b01-48c7-b89b-fc54057c38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value\n",
    "var_name = 'refkdt'\n",
    "var_value = '0'\n",
    "utils.change_config(fs, config_path, config_dict, var_name, var_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327eeb2-40cb-441e-ba3b-a0ce9b89cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view changes\n",
    "config_dict = utils.load_config(config_path, endpoint_url, fs)\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace53cca-b8ed-4e99-ab3e-f83ae458e2df",
   "metadata": {},
   "source": [
    "### Submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dabcae-8dcb-43dc-91df-f34cf5bc9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argo token, obtained from https://workflows.argo.cuahsi.io/userinfo \n",
    "ARGO_TOKEN=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c86cc-1fc2-4bf7-a73d-f4cdccf3bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of ArgoAPI using our ARGO_TOKEN\n",
    "argo = ArgoAPI(ARGO_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c22cc-ec3f-4726-a54d-894c9b895696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the metadata for the workflow\n",
    "argo.describe('ngen-run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d463db5c-b2df-4ca9-9931-9abb9015a40b",
   "metadata": {},
   "source": [
    "To run this workflow, we need to provide values for each of the input parameters listed above. In this example, the `input bucket` and `input data path` refer to the data that has already been subsetted, prepared, and used in Demo 1, which is stored on Minio S3. The `output bucket` is the same as the input bucket because we want to save the results in the same location. However, we will save the model outputs of the new simulation in a new `out put path` to prevent overwriting the results from Demo 1. This approach allows us to compare the results and see the impact of our modifications to the parameter of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28172e4-7fa1-46fc-8b0b-ab60277b8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"input-data-bucket\": bucket_name,\n",
    "    \"input-data-path\": data_path,\n",
    "    \"catchment-file-path\": \"domain/catchments.geojson\",\n",
    "    \"nexus-file-path\": \"domain/nexus.geojson\",\n",
    "    \"realization-file-path\": \"config/realization.json\",\n",
    "    \"output-bucket\": bucket_name,\n",
    "    \"output-path\": data_path+f'{cat}_{param}_0',\n",
    "}\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92092b-bc51-4006-9f75-92ac80cf007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = argo.submit_workflow('ngen-run', parameters)\n",
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccd407-6c08-4284-a684-70b4b583ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------------')\n",
    "print('| View the workflow in the Argo UI |')\n",
    "print('------------------------------------')\n",
    "print(f'https://workflows.argo.cuahsi.io/workflows/workflows/{job_name}\\n')\n",
    "\n",
    "print('----------------------')\n",
    "print('| Current Job Status |')\n",
    "print('----------------------')\n",
    "argo.workflow_status(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd9a4c-acda-4d5e-b031-877605346a47",
   "metadata": {},
   "source": [
    "To preview the output data that our job created, we need to first construct the url to our output data. Our output path use our hydroshare username so that we can easily find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b22e9b-e695-4449-b400-f4c4bd1f678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Browse the output files at:')\n",
    "print(f'https://console.minio.cuahsi.io/browser/{bucket_name}/{data_path}{cat}_{param}_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb555f6-bd70-48e0-a0b4-8a6a41273cc3",
   "metadata": {},
   "source": [
    "### Preview Simulation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fc286-e751-4bc0-94f6-adfa821130bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f's3://{bucket_name}/{data_path}'\n",
    "\n",
    "forcing_path = f'{url}forcing'\n",
    "base_results_path = f'{url}results'\n",
    "mod_results_path = f'{url}{cat}_{param}_0/results'\n",
    "print(f'forcing_path: {forcing_path}\\nbase results_path: {base_results_path}\\nmodified_results_path: {mod_results_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d071a-00e6-4774-a9cf-e68e87f5ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# load data from Demo 1 (base simulation)\n",
    "# connect to the CUAHSI MinIO server that is hosting our data\n",
    "s3 = fsspec.filesystem(\"s3\",\n",
    "                       anon=True,\n",
    "                       client_kwargs={'endpoint_url':'https://api.minio.cuahsi.io'},\n",
    "                       use_listings_cache=False,\n",
    "                      )\n",
    "\n",
    "forcing_csv_files = s3.glob(f'{forcing_path}/cat*.csv')\n",
    "base_nex_csv_files = s3.glob(f'{base_results_path}/nex*.csv')\n",
    "base_cat_csv_files = s3.glob(f'{base_results_path}/cat*.csv')\n",
    "base_forcing_xr = utils.forcing_csv2xr(s3, forcing_csv_files)   # convert csv to xarray\n",
    "base_nex_xr =  utils.nex_csv2xr(s3, base_nex_csv_files)         # convert csv to xarray \n",
    "base_cat_xr = utils.cat_csv2xr(s3, base_cat_csv_files)          # convert csv to xarray \n",
    "\n",
    "############# load data from Demo 2 (modify parameters)\n",
    "mod_nex_csv_files = s3.glob(f'{mod_results_path}/nex*.csv')\n",
    "mod_cat_csv_files = s3.glob(f'{mod_results_path}/cat*.csv')\n",
    "mod_nex_xr =  utils.nex_csv2xr(s3, mod_nex_csv_files)           # convert csv to xarray\n",
    "mod_cat_xr = utils.cat_csv2xr(s3, mod_cat_csv_files)            # convert csv to xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8962e-29c8-4e08-bfaa-de66327d8c27",
   "metadata": {},
   "source": [
    "Create a plot of input preciptation and simulated streamflow for the base (Demo 1) and test (Demo 2) scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157778b3-103f-4dfb-967a-00a7521a1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_flow_comparison(Outlet_catchment_id, Outlet_nexus_id, \n",
    "                         base_forcing_xr, base_nex_xr, base_cat_xr, \n",
    "                         mod_nex_xr, mod_cat_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba028ebb-0bce-4266-b156-4e65fa1d2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xarray datasets as netcdf files to your local environment\n",
    "base_forcing_xr.to_netcdf('./forcing.nc')\n",
    "base_nex_xr.to_netcdf('./base_simulation_nexus_results.nc')\n",
    "base_cat_xr.to_netcdf('./base_simulation_cat_results.nc')\n",
    "mod_nex_xr.to_netcdf('./test_simulation_xr_results.nc') \n",
    "mod_cat_xr.to_netcdf('./test_simulation_cat_results.nc') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf8805-9b9e-4741-a0f7-f258680dfd99",
   "metadata": {},
   "source": [
    "Upload files to the S3-compatible object storage service where your model inputs and outputs are stored. Currently, we have set `signature_version=UNSIGNED`, which means you do not require authentication and can use unsigned requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b429e-e923-4be5-8fb9-fa045d3c4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import glob\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "s3 = boto3.client('s3', endpoint_url=\"https://api.minio.cuahsi.io\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "for file in glob.glob('./*.nc'):\n",
    "    s3.upload_file(file, bucket_name, f'{data_path}post_process/{os.path.basename(file)}')\n",
    "\n",
    "print(f'https://console.minio.cuahsi.io/browser/{bucket_name}/{data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62927800-5dcb-4e4a-89bd-6328b4289f3d",
   "metadata": {},
   "source": [
    "### Share Results on HydroShare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed7f31-a658-45df-a613-78bee0786f65",
   "metadata": {},
   "source": [
    "Save the data of the modeling and analysis results on HydroShare. Use the HydroShare Python Client (hsclient), a library that allows users to interact with HydroShare, to create a new resource for your analysis datasets. For more information, see the [hsclient GitHub page](https://github.com/hydroshare/hsclient) and this [HydroShare resource](https://www.hydroshare.org/resource/7561aa12fd824ebb8edbee05af19b910/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30c708-28cf-472f-a1d7-7ff05e9a8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsclient import HydroShare\n",
    "\n",
    "# sign in to the system using your HydroShare credentials\n",
    "hs = HydroShare()\n",
    "hs.sign_in()\n",
    "\n",
    "# create a resource\n",
    "res = hs.create()\n",
    "\n",
    "# define metadata (including title, abstract, keywords)\n",
    "res.metadata.title = 'Comparison of CFE Model Outputs to Assess the Impact of Modifying Soil Hydraulic Conductivity on Streamflow'\n",
    "res.metadata.abstract = 'This resource provides links to the comparative analysis of two distinct CFE simulation scenarios. \\\n",
    "    The first scenario, referred to as the \"base\" scenario, utilizes default model parameters. \\\n",
    "    The second scenario, named the \"test\" scenario, uses a modified `refkdt` (reference hydraulic conductivity). \\\n",
    "    All relevant model inputs, configurations, outputs, and post-processing results for this comparative\\\n",
    "    analysis are sored in S3 storage.'\n",
    "res.metadata.subjects = ['argo workflows', 'cfe model outputs', 'hydraulic conductivity', 'CIROH developers conference 2024']\n",
    "\n",
    "# Call the save function to save the metadata edits to HydroShare\n",
    "res.save()\n",
    "\n",
    "# Creates a HydroShare reference object to reference content outside of the resource\n",
    "res.reference_create(\"s3reference\", MinIO_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fa6c3-5ff2-43c9-a962-518ed644b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Your new resource is available at: {res.metadata.url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d01a-23d4-4fe0-a056-6cb2d02c0fa5",
   "metadata": {},
   "source": [
    "Add more metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7ba33-f1d9-4f1f-99ea-69dc600f9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsmodels.schemas.fields import PeriodCoverage\n",
    "\n",
    "# Create a beginning and ending date for a time period\n",
    "beginDate = pd.to_datetime(base_forcing_xr.time[0].values)\n",
    "endDate = pd.to_datetime(base_forcing_xr.time[-1].values)\n",
    "\n",
    "# Set the temporal coverage of the resource to a PeriodCoverage object\n",
    "res.metadata.period_coverage = PeriodCoverage(start=beginDate, end=endDate)\n",
    "\n",
    "# Save the changes to the resource in HydroShare\n",
    "res.save()\n",
    "print(f'Your new resource is available at: {res.metadata.url}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
